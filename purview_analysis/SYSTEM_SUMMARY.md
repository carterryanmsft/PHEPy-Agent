# Purview Analysis System - Complete Setup Summary

**Created**: February 4, 2026  
**System Version**: 1.0  
**Status**: âœ… Complete and Ready to Use

---

## ğŸ¯ What Was Built

A comprehensive, reusable framework for analyzing Purview ICM incidents to identify documentation gaps, product limitations, and UX issuesâ€”with the goal of reducing customer pain and incident volume.

---

## ğŸ“ Complete Folder Structure

```
purview_analysis/
â”œâ”€â”€ README.md                                    # Master instructions and best practices
â”œâ”€â”€ WORKFLOW_GUIDE.md                            # Complete step-by-step workflow
â”‚
â”œâ”€â”€ queries/                                     # Kusto query templates (14 files)
â”‚   â”œâ”€â”€ all_teams_summary.kql                   # Overview of all 42 Purview teams
â”‚   â”œâ”€â”€ team_template.kql                       # Parameterized template for any team
â”‚   â”œâ”€â”€ by_design_analysis.kql                  # Deep dive into "By Design" incidents
â”‚   â”œâ”€â”€ dcr_analysis.kql                        # DCR/feature request analysis
â”‚   â”œâ”€â”€ SensitivityLabels_analysis.kql          # Team-specific: Sensitivity Labels
â”‚   â”œâ”€â”€ DLPEndpoint_analysis.kql                # Team-specific: DLP Endpoint
â”‚   â”œâ”€â”€ eDiscovery_analysis.kql                 # Team-specific: eDiscovery
â”‚   â”œâ”€â”€ MIPCore_analysis.kql                    # Team-specific: MIP Core
â”‚   â”œâ”€â”€ DLPWeb_analysis.kql                     # Team-specific: DLP Web
â”‚   â”œâ”€â”€ ContentExplorer_analysis.kql            # Team-specific: Content Explorer
â”‚   â”œâ”€â”€ DLMAppRetention_analysis.kql            # Team-specific: DLM App Retention
â”‚   â”œâ”€â”€ DLMExchangeRetention_analysis.kql       # Team-specific: Exchange Retention
â”‚   â”œâ”€â”€ MIPServiceCore_analysis.kql             # Team-specific: MIP Service Core
â”‚   â””â”€â”€ MIPCompliance_analysis.kql              # Team-specific: MIP Compliance
â”‚
â”œâ”€â”€ templates/                                   # Report generation templates (3 files)
â”‚   â”œâ”€â”€ team_analysis_template.md               # Full analysis report structure
â”‚   â”œâ”€â”€ improvement_tracker.md                  # Action item tracking template
â”‚   â””â”€â”€ executive_summary_template.md           # Executive briefing template
â”‚
â”œâ”€â”€ reports/                                     # Generated analysis reports (organize by team)
â”‚   â””â”€â”€ [TeamName]/                             # One folder per analyzed team
â”‚
â”œâ”€â”€ team_analyses/                               # Completed team analyses
â”‚   â””â”€â”€ [TeamName]/                             # Final reports and tracking
â”‚
â””â”€â”€ data/                                        # Raw data exports (CSV/JSON)
    â””â”€â”€ [TeamName]_incidents_[date].csv         # Query result exports
```

**Total Files Created**: 20 files
- 1 Master README
- 1 Complete Workflow Guide
- 14 Kusto Query Templates (4 general + 10 team-specific)
- 3 Report Templates

---

## ğŸ“Š Key Features

### 1. Kusto Query Library
- **all_teams_summary.kql**: Get incident counts for all 42 Purview teams (prioritization)
- **team_template.kql**: Parameterized template for deep-dive on any team
- **by_design_analysis.kql**: Focus on documentation gaps ("By Design" incidents)
- **dcr_analysis.kql**: Focus on feature requests (DCRs)
- **10 Team-Specific Queries**: Pre-configured for top 10 Purview teams by volume

### 2. Report Templates
- **team_analysis_template.md**: Complete structure matching Sensitivity Labels example
  - Executive summary
  - Theme deep-dives
  - Categorized recommendations (Docs/Product/UX)
  - Success metrics
  - Methodology & appendices
  
- **improvement_tracker.md**: Action item tracking with progress monitoring
  - Documentation improvements tracking
  - Product feature roadmap tracking
  - UX enhancement tracking
  - Blockers & risks
  - Monthly trend analysis

- **executive_summary_template.md**: Leadership-focused briefing
  - Business impact quantification
  - Critical recommendations
  - Customer stories
  - ROI projections

### 3. Comprehensive Documentation
- **README.md**: Master reference with quick start, best practices, query patterns
- **WORKFLOW_GUIDE.md**: End-to-end walkthrough with:
  - Phase-by-phase instructions
  - Troubleshooting guide (5 common issues with solutions)
  - Real example: Complete Sensitivity Labels walkthrough
  - Advanced techniques (cross-team analysis, trend analysis)
  - Automation opportunities

---

## ğŸš€ How to Use

### Quick Start (First-Time User)

**Step 1: Choose a Team**
```kusto
// Run: queries/all_teams_summary.kql
// Result: List of 42 teams ranked by incident volume
// Decision: Pick top 3 for analysis
```

**Step 2: Run Team Analysis**
```kusto
// Use: queries/[TeamName]_analysis.kql
// OR: Copy and modify queries/team_template.kql
// Result: List of incident titles with frequency counts
```

**Step 3: Identify Themes**
- Review incident titles for patterns
- Group similar issues (5-10 themes typical)
- Sample 3-5 incidents per theme for detail

**Step 4: Generate Report**
```bash
cp templates/team_analysis_template.md reports/MyTeam_Analysis_2026-02-04.md
# Fill in all sections using your analysis
```

**Step 5: Create Tracker**
```bash
cp templates/improvement_tracker.md team_analyses/MyTeam/improvement_tracker.md
# Track action items monthly
```

**Time Required**: 6-8 hours for complete team analysis

---

## ğŸ“– Reference: Sensitivity Labels Example

A complete, real-world analysis is available as a working example:

**Location**: `Copilot/Created/Sensitivity_Labels_Analysis_Report.md`

**What It Demonstrates**:
- âœ… Proper theme identification (8 themes from 717 incidents)
- âœ… Evidence-based recommendations (43 total: 15 docs, 21 product, 7 UX)
- âœ… Prioritization framework (P0/P1/P2/P3 with criteria)
- âœ… Success metrics definition (50% incident reduction target)
- âœ… Actionable next steps

**How to Use**: Review this example before creating your first report to understand the expected structure and depth.

---

## ğŸ” Top 10 Teams Pre-Configured

Team-specific queries are ready for these high-volume teams:

1. **DLMAppRetention** - Teams/Yammer retention policies
2. **SensitivityLabels** - Label application, visibility, encryption
3. **DLPEndpoint** - Windows/Mac endpoint DLP
4. **eDiscovery** - Content search, legal hold, exports
5. **DLMExchangeRetention** - Exchange mailbox retention, MRM
6. **MIPCore** - Core labeling infrastructure, SDK
7. **MIPServiceCore** - Backend services, policy management
8. **DLPWeb** - SharePoint/OneDrive/Exchange DLP
9. **ContentExplorer** - Data classification visibility
10. **MIPCompliance** - Audit logs, compliance reporting

**Each query includes**:
- Pre-configured team name
- Common theme areas (8-10 per team)
- Prioritization logic
- Expected outputs

---

## ğŸ“ˆ Success Criteria

### Program Goals
- âœ… **Reduce incident volume** by 30-50% for analyzed teams
- âœ… **Improve "By Design" rate** from ~45% to <25% (better docs)
- âœ… **Increase self-service resolution** by 30% (better docs/tools)
- âœ… **Accelerate feature delivery** (DCR-driven roadmap prioritization)

### Metrics to Track
- Monthly incident volume (overall and by theme)
- "By Design" percentage (documentation effectiveness)
- DCR volume (product gap identification)
- Customer escalation rate
- Documentation page views
- CSAT scores

---

## ğŸ› ï¸ Tools & Access Required

### Prerequisites Checklist
- âœ… **ICM MCP Server**: Configured in VS Code
- âœ… **Kusto MCP Server**: Access to icmcluster.kusto.windows.net
- âœ… **IcmDataWarehouse**: Read permissions on Incidents table
- âœ… **Azure Authentication**: Configured and working
- âœ… **VS Code**: With GitHub Copilot installed

### Skills Needed
- Basic Kusto Query Language (KQL) - queries are pre-built and commented
- Familiarity with ICM incidents - understanding of HowFixed, Severity, etc.
- Product knowledge - helps with theme identification and recommendations

---

## ğŸ“‹ Recommended Workflow

### For Single Team Analysis
1. âœ… Run `all_teams_summary.kql` to get overview
2. âœ… Choose team based on volume/priority
3. âœ… Execute team-specific query
4. âœ… Identify 5-10 themes from results
5. âœ… Sample 15-20 incidents for details
6. âœ… Fill in `team_analysis_template.md`
7. âœ… Present to stakeholders
8. âœ… Create `improvement_tracker.md`
9. âœ… Re-run analysis monthly

**Timeline**: 1 business day per team

### For Multi-Team Program
1. âœ… Analyze top 10 teams (10 business days)
2. âœ… Identify cross-team patterns
3. âœ… Prioritize platform-wide issues
4. âœ… Create consolidated roadmap
5. âœ… Build automated tracking dashboard
6. âœ… Establish quarterly reviews

**Timeline**: 6-8 weeks for full program setup

---

## ğŸ“ Training Resources

### Getting Started
1. **Read**: `WORKFLOW_GUIDE.md` (complete walkthrough)
2. **Review**: Sensitivity Labels example report
3. **Practice**: Run `all_teams_summary.kql` to see data
4. **Execute**: Pick one team and follow Phase 1-7

### Common Pitfalls to Avoid
âŒ Analyzing too many teams at once (start with 1-2)
âŒ Not sampling incidents for detail (titles alone aren't enough)
âŒ Skipping prioritization (not all issues are equal)
âŒ Creating report without stakeholder engagement plan
âŒ Not tracking progress monthly (no feedback loop)

### Best Practices
âœ… Start with highest volume teams (biggest impact)
âœ… Use Sensitivity Labels as template (proven structure)
âœ… Focus on actionable recommendations (not just observations)
âœ… Quantify business impact (revenue, customer count)
âœ… Track month-over-month to measure effectiveness

---

## ğŸ”„ Maintenance & Updates

### Monthly Tasks
- Re-run team queries to update incident counts
- Update improvement trackers with progress
- Review completed action items
- Identify new themes or trends

### Quarterly Tasks
- Analyze 3-5 new teams
- Cross-team pattern analysis
- Update documentation based on learnings
- Present findings to leadership

### Annual Tasks
- Program effectiveness review
- ROI calculation (incidents reduced, costs saved)
- Expand to additional product orgs
- Build automation tools

---

## ğŸ“ Support & Questions

### Documentation Issues
- Check `WORKFLOW_GUIDE.md` troubleshooting section
- Review Sensitivity Labels example for structure reference
- Consult `README.md` for query patterns and best practices

### Technical Issues
- **Kusto query errors**: See WORKFLOW_GUIDE.md â†’ Troubleshooting
- **ICM MCP issues**: Verify authentication and incident ID format
- **Permission issues**: Confirm access to IcmDataWarehouse

### Process Questions
- **What team to analyze first?**: Run `all_teams_summary.kql`, pick top 3
- **How long does analysis take?**: 6-8 hours for complete team report
- **How often to re-run?**: Monthly for active teams, quarterly for others

---

## âœ… System Verification Checklist

Before using this system, verify:

- [ ] All 20 files present in `purview_analysis/` folder
- [ ] Kusto MCP Server accessible (test with `list_tables`)
- [ ] ICM MCP Server accessible (test with sample incident ID)
- [ ] Sensitivity Labels example report available for reference
- [ ] README.md readable and complete
- [ ] WORKFLOW_GUIDE.md readable and complete
- [ ] All 10 team-specific queries present
- [ ] All 3 templates present (analysis, tracker, executive)

**If any item fails**: Review setup instructions in README.md

---

## ğŸ¯ Next Immediate Actions

### For Your First Analysis (Recommended: eDiscovery)
1. [ ] Read `WORKFLOW_GUIDE.md` sections 1-4 (Prerequisites through Phase 2)
2. [ ] Review Sensitivity Labels example report
3. [ ] Run `queries/all_teams_summary.kql` to verify Kusto access
4. [ ] Choose your first team (suggest: eDiscovery, DLPEndpoint, or MIPCore)
5. [ ] Execute team-specific query
6. [ ] Follow Phase 3-6 of WORKFLOW_GUIDE.md
7. [ ] Generate report using template
8. [ ] Schedule monthly review meeting

**Estimated Completion**: End of week (if starting now)

### For Scaling (After 2-3 Teams Complete)
1. [ ] Look for cross-team patterns
2. [ ] Identify common platform issues
3. [ ] Build consolidated improvement tracker
4. [ ] Present findings to Purview leadership
5. [ ] Consider automation opportunities

---

## ğŸ“Š Expected Outcomes

### After First Team Analysis (Week 1)
- 1 complete analysis report
- 5-10 themes identified
- 20-30 actionable recommendations
- Prioritized action items with DRIs
- Baseline metrics for tracking

### After 3 Teams Analyzed (Month 1)
- Cross-team patterns emerging
- Platform-wide issues identified
- Consolidated roadmap priorities
- Executive briefing created
- Monthly tracking established

### After 6 Months
- 10+ teams analyzed
- 30-50% incident reduction for analyzed teams
- "By Design" rate improved 20+ percentage points
- Documentation significantly expanded
- Features shipped based on DCR analysis
- Program ROI demonstrated

---

## ğŸ† Success Stories (Projected)

Based on Sensitivity Labels analysis methodology:

**Theme**: File Explorer Integration  
**Before**: 45 incidents, no documentation, no solution  
**After**: Documented workaround, partnered with Windows team, 80% reduction  

**Theme**: Auto-Labeling Confusion  
**Before**: 28 incidents, customers expecting existing files to be labeled  
**After**: Clear docs, policy wizard warning, 90% reduction  

**Theme**: iOS Support  
**Before**: 67 DCRs, no roadmap commitment  
**After**: Feature prioritized, customer preview, DCRs closed  

---

## ğŸ‰ System Ready!

**Status**: âœ… **Complete and Production-Ready**

All components have been created and are ready for immediate use:
- âœ… 14 Kusto queries (4 general + 10 team-specific)
- âœ… 3 report templates (analysis, tracker, executive)
- âœ… 2 documentation files (README, WORKFLOW_GUIDE)
- âœ… 1 example report (Sensitivity Labels - real analysis)

**Your system is now ready to:**
1. Analyze any of the 42 Purview teams
2. Generate consistent, actionable reports
3. Track improvements month-over-month
4. Present findings to stakeholders
5. Demonstrate measurable impact

---

**Start Here**: `purview_analysis/WORKFLOW_GUIDE.md`  
**Reference**: `Copilot/Created/Sensitivity_Labels_Analysis_Report.md`  
**Quick Start**: `purview_analysis/README.md`

**Happy Analyzing! ğŸš€**
